{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noob2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import multiprocessing\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import gensim.models.word2vec as w2v\n",
    "from gensim.models import Phrases\n",
    "from collections import Counter\n",
    "\n",
    "import sklearn.manifold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shu/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['seed']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Download NLTK tokenizer models (only the first time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/shu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/shu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load books from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_filenames = sorted(glob.glob(\"data/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books: ['data/*.txt']\n"
     ]
    }
   ],
   "source": [
    "print(\"Found books:\",data_filenames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine the books into one string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'data/*.txt'...\n",
      "Corpus is now 194670485 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for data_filename in data_filenames:\n",
    "    print(\"Reading '{0}'...\".format(data_filename))\n",
    "    with codecs.open(data_filename, \"r\", \"utf-8\") as data_file:\n",
    "        corpus_raw += data_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the corpus into sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial_raw_sentences = raw_sentences[0:50] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#partial_raw_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stopset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleanups = [token.lower() for token in raw_sentences if token.lower() not in stopset and len(token)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the sentense to words level and cleanup stopwords from raw_sentences, \n",
    "cleanups = []\n",
    "for token in partial_raw_sentences:\n",
    "    cleanup = []\n",
    "    lower_token = token.lower()\n",
    "    words_list = lower_token.split()\n",
    "    for word in words_list:\n",
    "        if word not in stopset:\n",
    "            cleanup.append(word)\n",
    "    cleanups.append(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'body\\n\"nah fam we good\\n\\n\"\\n\"ok but without caffeine my friend isn\\'t motivated to even go out for a run, he just gets sleepy.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing for cleaning the stopword\n",
    "partial_raw_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['body',\n",
       " '\"nah',\n",
       " 'fam',\n",
       " 'good',\n",
       " '\"',\n",
       " '\"ok',\n",
       " 'without',\n",
       " 'caffeine',\n",
       " 'friend',\n",
       " 'motivated',\n",
       " 'even',\n",
       " 'go',\n",
       " 'run,',\n",
       " 'gets',\n",
       " 'sleepy.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 497 tokens\n"
     ]
    }
   ],
   "source": [
    "#count for the token\n",
    "token_count = sum([len(cleanup1) for cleanup1 in cleanups])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-18 14:20:54,049 : INFO : collecting all words and their counts\n",
      "2018-01-18 14:20:54,052 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-18 14:20:54,056 : INFO : collected 797 word types from a corpus of 497 words (unigram + bigrams) and 50 sentences\n",
      "2018-01-18 14:20:54,059 : INFO : using 797 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2018-01-18 14:20:54,063 : INFO : source_vocab length 797\n",
      "2018-01-18 14:20:54,073 : INFO : Phraser built with 0 0 phrasegrams\n"
     ]
    }
   ],
   "source": [
    "#??? still a bit confused about the phrases function and the bigram data type\n",
    "from gensim.models.phrases import Phraser\n",
    "phrases = Phrases(cleanups)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-18 14:20:56,804 : INFO : collecting all words and their counts\n",
      "2018-01-18 14:20:56,816 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-01-18 14:20:56,827 : INFO : collected 797 word types from a corpus of 497 words (unigram + bigrams) and 50 sentences\n",
      "2018-01-18 14:20:56,828 : INFO : using 797 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "trigram = Phrases(bigram[cleanups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fleur', 'de', 'peau', 'new', 'york', 'lunar', 'elite', 'hello', 'me']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shu/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "sent = [u'fleur', u'de', u'peau', u'new', u'york', u'lunar', u'elite', u'hello', u'me']\n",
    ">>> print((trigram[bigram[sent]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shu/anaconda3/lib/python3.6/site-packages/gensim/models/phrases.py:486: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "new_stuff= list((trigram[bigram[cleanups]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['body',\n",
       "  '\"nah',\n",
       "  'fam',\n",
       "  'good',\n",
       "  '\"',\n",
       "  '\"ok',\n",
       "  'without',\n",
       "  'caffeine',\n",
       "  'friend',\n",
       "  'motivated',\n",
       "  'even',\n",
       "  'go',\n",
       "  'run,',\n",
       "  'gets',\n",
       "  'sleepy.'],\n",
       " ['maybe', 'overdid', 'speed?'],\n",
       " ['mean',\n",
       "  'noticed',\n",
       "  'performance',\n",
       "  'improvement',\n",
       "  'caffeine',\n",
       "  'compared',\n",
       "  'stuff,',\n",
       "  'feels',\n",
       "  'useless',\n",
       "  'without',\n",
       "  'stimulants.'],\n",
       " [\"can't\",\n",
       "  'get',\n",
       "  'stuff',\n",
       "  'done,',\n",
       "  'brain',\n",
       "  'work',\n",
       "  \"what's\",\n",
       "  'good',\n",
       "  'balance?'],\n",
       " ['drop', 'amphetamines?'],\n",
       " ['caffeine', 'pill', 'better', 'cup', 'coffee?\"'],\n",
       " ['\"when',\n",
       "  'xc',\n",
       "  'coach',\n",
       "  'said',\n",
       "  'eat',\n",
       "  'running',\n",
       "  'cause',\n",
       "  'would',\n",
       "  'get',\n",
       "  'cramps',\n",
       "  'eat',\n",
       "  '4',\n",
       "  'hours',\n",
       "  'running',\n",
       "  'amphetamine',\n",
       "  'powder,',\n",
       "  'kind',\n",
       "  'yellowish-brown,',\n",
       "  'think',\n",
       "  'mostly',\n",
       "  'pure.'],\n",
       " ['eating', 'running', 'work', 'you?'],\n",
       " ['usually',\n",
       "  'eat',\n",
       "  'long',\n",
       "  'runs\"',\n",
       "  'barefoot',\n",
       "  'sounds....',\n",
       "  'interesting',\n",
       "  'o_o',\n",
       "  'friend',\n",
       "  'similar',\n",
       "  'story',\n",
       "  'except',\n",
       "  'winter',\n",
       "  'alaska',\n",
       "  'retarded',\n",
       "  'boy',\n",
       "  'running',\n",
       "  'asked',\n",
       "  'sprayed',\n",
       "  'got',\n",
       "  'blasted.'],\n",
       " ['asics', 'way.'],\n",
       " ['gt', '1000', '2000s', 'best,', 'even', 'prefer', 'kayano.'],\n",
       " ['take',\n",
       "  'medical',\n",
       "  'advice',\n",
       "  'anecdotal',\n",
       "  'evidence',\n",
       "  'reddit',\n",
       "  'medical',\n",
       "  'advice.'],\n",
       " ['really',\n",
       "  'hesitant',\n",
       "  'seeing',\n",
       "  'doctor',\n",
       "  '(presumably',\n",
       "  'insurance',\n",
       "  'cost',\n",
       "  'time?)'],\n",
       " ['best',\n",
       "  'bet',\n",
       "  'would',\n",
       "  'talking',\n",
       "  'coach,',\n",
       "  'maybe',\n",
       "  'physical',\n",
       "  'therapist',\n",
       "  'perhaps',\n",
       "  'school',\n",
       "  '(university?)'],\n",
       " ['free', 'access', 'clinic/physical', 'therapist', 'might', 'know', 'of?'],\n",
       " ['mean?'],\n",
       " ['5k', '20', 'impossible', 'guys', 'mile', 'less', '4', 'minutes.'],\n",
       " ['\"&gt;',\n",
       "  'hand,',\n",
       "  'lifting',\n",
       "  '3-6',\n",
       "  'times',\n",
       "  'week',\n",
       "  'past',\n",
       "  '3',\n",
       "  'years',\n",
       "  'little',\n",
       "  'progress.'],\n",
       " ['know',\n",
       "  'mainly',\n",
       "  'fault',\n",
       "  'since',\n",
       "  'eat',\n",
       "  'much',\n",
       "  'supposed',\n",
       "  'order',\n",
       "  'gain',\n",
       "  'muscle,',\n",
       "  'time',\n",
       "  \"i'm\",\n",
       "  'sorry,',\n",
       "  'honestly',\n",
       "  'mean',\n",
       "  'sound',\n",
       "  'like',\n",
       "  'asshole',\n",
       "  'here,',\n",
       "  'perpetuating',\n",
       "  'misconception.'],\n",
       " ['\"\"progress\"\"',\n",
       "  'lifting,',\n",
       "  'measured',\n",
       "  '\"\"how',\n",
       "  'much',\n",
       "  'lift\"\"',\n",
       "  'bound',\n",
       "  'muscle',\n",
       "  'mass',\n",
       "  'beginning',\n",
       "  'lifters',\n",
       "  '-',\n",
       "  'adaptations',\n",
       "  'lie',\n",
       "  'elsewhere',\n",
       "  '-',\n",
       "  'short,',\n",
       "  'good',\n",
       "  'recruiting',\n",
       "  'muscle',\n",
       "  'fibre',\n",
       "  '(neurological',\n",
       "  'adaptations),',\n",
       "  'optimal',\n",
       "  'movement',\n",
       "  'patterns',\n",
       "  '(form/technique).'],\n",
       " ['look',\n",
       "  'training',\n",
       "  'program',\n",
       "  'training',\n",
       "  'plans',\n",
       "  'looking',\n",
       "  'lifting',\n",
       "  'progress,',\n",
       "  'rather',\n",
       "  'adding',\n",
       "  '~~weight~~',\n",
       "  'body',\n",
       "  'mass.'],\n",
       " ['doubly',\n",
       "  'want',\n",
       "  'combine',\n",
       "  'running,',\n",
       "  'every',\n",
       "  'kg',\n",
       "  'carry',\n",
       "  'unneccesarily',\n",
       "  'going',\n",
       "  'slow',\n",
       "  'down.\"'],\n",
       " ['\"i',\n",
       "  'recognize',\n",
       "  'company',\n",
       "  'willing',\n",
       "  'replace',\n",
       "  'socks,',\n",
       "  'wearing',\n",
       "  'holes',\n",
       "  'them,',\n",
       "  'buy',\n",
       "  'new',\n",
       "  'ones.'],\n",
       " ['abusing', 'generous', 'guarantee.', '\"'],\n",
       " ['repeatedly',\n",
       "  'destroying',\n",
       "  'socks',\n",
       "  '&lt;3',\n",
       "  'months',\n",
       "  'use,',\n",
       "  'think',\n",
       "  'using',\n",
       "  'guarantee.'],\n",
       " ['\"is', 'first', 'time', 'wearing', 'spikes?'],\n",
       " ['first', 'time', 'used', 'spikes', 'race?'],\n",
       " ['even',\n",
       "  'latter',\n",
       "  'case',\n",
       "  'sounds',\n",
       "  'like',\n",
       "  'used',\n",
       "  'running',\n",
       "  'spikes,',\n",
       "  'suspect',\n",
       "  'normally',\n",
       "  'run',\n",
       "  'rearfoot',\n",
       "  'strike',\n",
       "  '(possibly',\n",
       "  'shoes',\n",
       "  'substantial',\n",
       "  'heel',\n",
       "  'drop).'],\n",
       " ['takes', 'time', 'get', 'used', 'running', 'spikes.'],\n",
       " ['especially',\n",
       "  'normally',\n",
       "  'run',\n",
       "  'rearfoot',\n",
       "  'strike,',\n",
       "  'since',\n",
       "  'land',\n",
       "  'fore/midfoot.'],\n",
       " ['&amp;nbsp;',\n",
       "  'illustrate,',\n",
       "  'land',\n",
       "  'forefoot',\n",
       "  'feet',\n",
       "  'plantar',\n",
       "  'flexed',\n",
       "  '(http://teachmeanatomy.info/wp-content/uploads/terms-of-movement-dorsiflexion-and-plantar-flexion-cc.jpg).'],\n",
       " ['due',\n",
       "  'plantar',\n",
       "  'flexion',\n",
       "  'able',\n",
       "  'store',\n",
       "  'much',\n",
       "  'elastic',\n",
       "  'energy',\n",
       "  'muscles,',\n",
       "  'ligaments',\n",
       "  'tendons,',\n",
       "  'especially',\n",
       "  'achilles.'],\n",
       " ['good', '\"\"bad\"\".'],\n",
       " ['good',\n",
       "  'energy',\n",
       "  'store',\n",
       "  'landing',\n",
       "  'returned',\n",
       "  'push',\n",
       "  'off,',\n",
       "  'like',\n",
       "  'spring',\n",
       "  'compress',\n",
       "  'release',\n",
       "  'bouncy',\n",
       "  'ball',\n",
       "  'drop',\n",
       "  'floor.'],\n",
       " ['words,',\n",
       "  'generate',\n",
       "  'less',\n",
       "  'energy',\n",
       "  'since',\n",
       "  '\"\"springs\"\"',\n",
       "  'body',\n",
       "  'already',\n",
       "  'return',\n",
       "  'energy.'],\n",
       " ['though',\n",
       "  'extra',\n",
       "  'energy',\n",
       "  'also',\n",
       "  'puts',\n",
       "  'stress',\n",
       "  'muscles,',\n",
       "  'ligaments',\n",
       "  'tendons,',\n",
       "  'hence',\n",
       "  'cause',\n",
       "  'pain',\n",
       "  'injuries',\n",
       "  'used',\n",
       "  'this,',\n",
       "  '\"\"bad\"\".'],\n",
       " ['used',\n",
       "  'running',\n",
       "  'forefoot/midfoot',\n",
       "  'strike,',\n",
       "  'train',\n",
       "  'get',\n",
       "  'used',\n",
       "  'train',\n",
       "  'spikes.'],\n",
       " ['&amp;nbsp;',\n",
       "  'another',\n",
       "  'thing',\n",
       "  'may',\n",
       "  'contributed',\n",
       "  'said',\n",
       "  'run',\n",
       "  'heavy',\n",
       "  'plantar',\n",
       "  'flexion',\n",
       "  '(dorsal',\n",
       "  'flex',\n",
       "  'really',\n",
       "  'make',\n",
       "  'sense',\n",
       "  'here).'],\n",
       " ['seems',\n",
       "  'suggest',\n",
       "  'constantly',\n",
       "  'ran',\n",
       "  '\"\"toes\"\"',\n",
       "  'hardly',\n",
       "  'let',\n",
       "  'heels',\n",
       "  'touch',\n",
       "  'ground.'],\n",
       " ['running',\n",
       "  'forefoot',\n",
       "  'without',\n",
       "  'heels',\n",
       "  'touch',\n",
       "  'ground',\n",
       "  'good',\n",
       "  'sprinting,',\n",
       "  'short',\n",
       "  'distance',\n",
       "  'put',\n",
       "  'lot',\n",
       "  'energy',\n",
       "  'short',\n",
       "  'amount',\n",
       "  'time.'],\n",
       " ['running', 'longer', 'distance', 'would', 'put', 'much', 'stress', 'body.'],\n",
       " ['hence', 'may', 'contributed', 'achilles', 'sore.'],\n",
       " ['&amp;nbsp;',\n",
       "  'advice',\n",
       "  'would',\n",
       "  'be,',\n",
       "  'get',\n",
       "  'used',\n",
       "  'running',\n",
       "  'spikes',\n",
       "  'pay',\n",
       "  'attention',\n",
       "  'technique',\n",
       "  '(note',\n",
       "  'take',\n",
       "  'rush',\n",
       "  'it).'],\n",
       " ['want',\n",
       "  'run',\n",
       "  'fore/midfoot',\n",
       "  'strike',\n",
       "  'allowing',\n",
       "  'heels',\n",
       "  'make',\n",
       "  'contact',\n",
       "  'ground,',\n",
       "  'stay',\n",
       "  'forefoot',\n",
       "  'entire',\n",
       "  'time',\n",
       "  '(and',\n",
       "  'run',\n",
       "  'toes).'],\n",
       " ['take',\n",
       "  'time',\n",
       "  'achilles',\n",
       "  'recover',\n",
       "  'though',\n",
       "  'keep',\n",
       "  'feeling',\n",
       "  'like',\n",
       "  'hell',\n",
       "  'go',\n",
       "  'see',\n",
       "  'doctor.'],\n",
       " ['last',\n",
       "  'thing',\n",
       "  'want',\n",
       "  'running',\n",
       "  'playing',\n",
       "  'basketball',\n",
       "  'serious',\n",
       "  'achilles',\n",
       "  'injury.'],\n",
       " ['also,', 'never', 'ran', 'spikes', 'race', 'best', 'place', 'start.'],\n",
       " ['uncomfortable',\n",
       "  'conditions',\n",
       "  'skip',\n",
       "  'race',\n",
       "  'start',\n",
       "  'training',\n",
       "  'spikes.'],\n",
       " ['though', 'still', 'applies', 'used', 'wearing', 'spikes.\"'],\n",
       " ['\"that', 'make', 'sense.']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors.\n",
    "num_features = 300\n",
    "\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 5\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 15\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2vec = w2v.Word2Vec(\n",
    "    sg=0,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-18 15:47:18,283 : INFO : collecting all words and their counts\n",
      "2018-01-18 15:47:18,287 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-01-18 15:47:18,289 : INFO : collected 363 word types from a corpus of 497 raw words and 50 sentences\n",
      "2018-01-18 15:47:18,291 : INFO : Loading a fresh vocabulary\n",
      "2018-01-18 15:47:18,294 : INFO : min_count=5 retains 7 unique words (1% of original 363, drops 356)\n",
      "2018-01-18 15:47:18,296 : INFO : min_count=5 leaves 46 word corpus (9% of original 497, drops 451)\n",
      "2018-01-18 15:47:18,299 : INFO : deleting the raw counts dictionary of 363 items\n",
      "2018-01-18 15:47:18,300 : INFO : sample=0.001 downsamples 7 most-common words\n",
      "2018-01-18 15:47:18,302 : INFO : downsampling leaves estimated 4 word corpus (9.0% of prior 46)\n",
      "2018-01-18 15:47:18,305 : INFO : estimated required memory for 7 words and 300 dimensions: 20300 bytes\n",
      "2018-01-18 15:47:18,308 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model2vec.build_vocab(new_stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(model2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start training, this might take a minute or two...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'corpus_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-aa1a99af6b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_stuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'corpus_count'"
     ]
    }
   ],
   "source": [
    "model2vec.train(new_stuff, total_examples=model2vec.corpus_count, epochs=model2vec.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to file, can be useful later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-18 15:48:25,352 : INFO : saving Word2Vec object under trained/guitar1.w2v, separately None\n",
      "2018-01-18 15:48:25,355 : INFO : not storing attribute syn0norm\n",
      "2018-01-18 15:48:25,358 : INFO : not storing attribute cum_table\n",
      "2018-01-18 15:48:25,364 : INFO : saved trained/guitar1.w2v\n"
     ]
    }
   ],
   "source": [
    "model2vec.save(os.path.join(\"trained\", \"guitar1.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-18 15:48:26,268 : INFO : storing 7x300 projection weights into guitar.bin\n"
     ]
    }
   ],
   "source": [
    "model2vec.wv.save_word2vec_format('guitar.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-18 15:49:37,128 : INFO : loading Word2Vec object from trained/furniture.w2v\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained/furniture.w2v'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-fb77220f7e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trained\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"furniture.w2v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1570\u001b[0m         \u001b[0;31m# update older models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     \u001b[0;34m\"\"\"Load pickled object from `fname`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained/furniture.w2v'"
     ]
    }
   ],
   "source": [
    "model2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"furniture.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decomposer():\n",
    "    def __init__(self, n_components=15, copy=True, whiten=True, mode=\"pca\", max_iter=2000, model=None):\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            if mode == \"pca\":\n",
    "                self.model = PCA(n_components=n_components, copy=copy, whiten=whiten)\n",
    "            elif mode == \"ica\":\n",
    "                self.model = FastICA(n_components=n_components, whiten=whiten, max_iter=max_iter)\n",
    "    def fit(self, X):\n",
    "        self.model.fit(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.model.transform(X)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        data = self.model.fit_transform(X)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decomposer = Decomposer(n_components=15)\n",
    "X_decomp = decomposer.fit_transform(model2vec.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HDBscanClusterer():\n",
    "    _min_cluster_size = 15\n",
    "    _metric = \"euclidean\"\n",
    "\n",
    "    def __init__(self, min_cluster_size=15, min_samples=1, alpha=1.0, metric=\"euclidean\"):\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.min_samples = min_samples\n",
    "        self.alpha = alpha\n",
    "        self.metric = metric\n",
    "        self.labels = []\n",
    "\n",
    "    @property\n",
    "    def min_cluster_size(self):\n",
    "        return self._min_cluster_size\n",
    "\n",
    "    @min_cluster_size.setter\n",
    "    def min_cluster_size(self, value):\n",
    "        self._min_cluster_size = value\n",
    "\n",
    "    @property\n",
    "    def metric(self):\n",
    "        return self._metric\n",
    "\n",
    "    @metric.setter\n",
    "    def metric(self, value):\n",
    "        self._metric = value\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @labels.setter\n",
    "    def labels(self, value):\n",
    "        self._labels = value\n",
    "\n",
    "    def cluster(self, X):\n",
    "        model = hdbscan.HDBSCAN(min_cluster_size=self.min_cluster_size, min_samples=self.min_samples, alpha=self.alpha, metric=self.metric)\n",
    "        labels = model.fit_predict(X)\n",
    "        self.labels = [{'index': i, 'cluster': j} for i, j in enumerate(labels)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "class L2Normalizer():\n",
    "    def norm(self, X):\n",
    "        return normalize(X, norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_decomp_norm = L2Normalizer().norm(X_decomp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.648851</td>\n",
       "      <td>-0.342814</td>\n",
       "      <td>-0.352010</td>\n",
       "      <td>0.257205</td>\n",
       "      <td>-0.320311</td>\n",
       "      <td>0.161088</td>\n",
       "      <td>0.377965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.137573</td>\n",
       "      <td>0.588912</td>\n",
       "      <td>-0.076517</td>\n",
       "      <td>0.034465</td>\n",
       "      <td>0.466079</td>\n",
       "      <td>0.516843</td>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.174164</td>\n",
       "      <td>-0.427684</td>\n",
       "      <td>0.376064</td>\n",
       "      <td>-0.066838</td>\n",
       "      <td>0.620961</td>\n",
       "      <td>-0.335280</td>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.675519</td>\n",
       "      <td>-0.449068</td>\n",
       "      <td>-0.146561</td>\n",
       "      <td>0.050394</td>\n",
       "      <td>-0.030337</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.098404</td>\n",
       "      <td>0.174321</td>\n",
       "      <td>-0.521193</td>\n",
       "      <td>-0.597560</td>\n",
       "      <td>-0.062925</td>\n",
       "      <td>-0.429409</td>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.245488</td>\n",
       "      <td>0.319209</td>\n",
       "      <td>0.063182</td>\n",
       "      <td>0.669410</td>\n",
       "      <td>-0.159364</td>\n",
       "      <td>-0.466353</td>\n",
       "      <td>0.377965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.058824</td>\n",
       "      <td>0.137124</td>\n",
       "      <td>0.657035</td>\n",
       "      <td>-0.347077</td>\n",
       "      <td>-0.514103</td>\n",
       "      <td>0.135719</td>\n",
       "      <td>0.377964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0 -0.648851 -0.342814 -0.352010  0.257205 -0.320311  0.161088  0.377965\n",
       "1 -0.137573  0.588912 -0.076517  0.034465  0.466079  0.516843  0.377964\n",
       "2 -0.174164 -0.427684  0.376064 -0.066838  0.620961 -0.335280  0.377964\n",
       "3  0.675519 -0.449068 -0.146561  0.050394 -0.030337  0.417391  0.377964\n",
       "4  0.098404  0.174321 -0.521193 -0.597560 -0.062925 -0.429409  0.377964\n",
       "5  0.245488  0.319209  0.063182  0.669410 -0.159364 -0.466353  0.377965\n",
       "6 -0.058824  0.137124  0.657035 -0.347077 -0.514103  0.135719  0.377964"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_decomp_norm)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdb = HDBscanClusterer(min_cluster_size=8, min_samples=1, alpha=0.25)\n",
    "hdb.cluster(X_decomp_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=1, alpha=0.25).fit(X_decomp_norm)\n",
    "clusterer.condensed_tree_.plot(select_clusters=True,\n",
    "selection_palette=sns.color_palette('deep', 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_decomp_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = hdb.labels\n",
    "vocab_map = model2vec.wv.vocab\n",
    "df = pd.DataFrame(res)\n",
    "df = df[df['cluster'] > -1]\n",
    "counts = df['cluster'].value_counts().to_dict()\n",
    "good = {k: v for k, v in counts.items() if v < 1000}\n",
    "df['ok'] = df['cluster'].apply(lambda x: x in good)\n",
    "df = df[df['ok']]\n",
    "n_clusters = len(df['cluster'].unique())\n",
    "print(\"FOUND {} CLUSTERS\".format(n_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame.from_dict(model2vec.wv.vocab, orient='index', dtype=None)\n",
    "data_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['raw_vector'] = df['index'].apply(lambda x: model2vec.wv.syn0[x])\n",
    "# df['word'] = df['index'].apply(lambda x: model2vec.wv.vocab[x])\n",
    "df['decomp_norm_vector'] = df['index'].apply(lambda x: X_decomp_norm[x])\n",
    "len(df.index)\n",
    "df.head(300)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('furniturehdb1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w in model2vec.wv.vocab:\n",
    "     print(w,model2vec.wv.vocab[w].count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame.from_dict(model2vec.wv.vocab, orient='index', dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_df.to_csv('guitarwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress the word vectors into 2D space and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0, perplexity=5.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix = model2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train t-SNE, this could take a minute or two...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the big picture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[model2vec.wv.vocab[word].index])\n",
    "            for word in model2vec.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pointsforcolor = pd.DataFrame(\n",
    "    [\n",
    "        (coords[0], coords[1])\n",
    "        for coords in [\n",
    "            (all_word_vectors_matrix_2d[model2vec.wv.vocab[word].index])\n",
    "            for word in model2vec.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=100)\n",
    "kmeans.fit(pointsforcolor)\n",
    "y_kmeans = kmeans.predict(pointsforcolor)\n",
    "pointsforcolor.plot.scatter(\"x\", \"y\", c=y_kmeans, s=3, cmap='viridis', figsize=(20, 15))\n",
    "points['kmeans'] = y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pointsforcolor.head(10)\n",
    "points.to_csv('ps4coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 15))\n",
    "plt.savefig('books_read.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Zoom in to some interesting places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points.to_csv('paintcoords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        \n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=15, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(-30, -10), y_bounds=(-30, -10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(0, 10), y_bounds=(40, 45))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore semantic similarities between book characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words closest to the given word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2vec.most_similar(\"bad\",[],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2vec.most_similar(\"xbox\",[],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2vec.most_similar(\"xbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear relationships between word pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = model2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"lather\", \"foamy\", \"fragance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
